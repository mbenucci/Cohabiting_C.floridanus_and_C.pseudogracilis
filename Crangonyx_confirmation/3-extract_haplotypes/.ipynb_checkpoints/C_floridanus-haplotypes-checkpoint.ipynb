{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract _C. floridanus_ and _C. pseudogracilis_ haplotypes from metabarcoding data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before extracting the _Crangonyx spp._ haplotypes we need to import the functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import metaBEAT_global_misc_functions as mb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mkdir Crangonyx_haplotypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cd Crangonyx_haplotypes/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now extract the OTU ids for the relevant species that we filtered above 2%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "OTU_table = mb.load_BIOM('../../2-metaBEAT/filtered.biom')\n",
    "\n",
    "dictionary = mb.find_target_OTUs_by_taxonomy(OTU_table, target='Crangonyx_floridanus', level='all')\n",
    "\n",
    "OTUs = {}\n",
    "OTUs['Crangonyx_floridanus'] = dictionary.keys()\n",
    "\n",
    "dictionary = mb.find_target_OTUs_by_taxonomy(OTU_table, target='Crangonyx_pseudogracilis', level='all')\n",
    "\n",
    "OTUs['Crangonyx_pseudogracilis'] = dictionary.keys()\n",
    "\n",
    "print \"\"\n",
    "print OTUs\n",
    "\n",
    "\n",
    "OTUs_as_list = []\n",
    "for sp in OTUs:\n",
    "    OTUs_as_list.extend(OTUs[sp])\n",
    "\n",
    "print \"\\nOTU list:\"\n",
    "print OTUs_as_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify samples contributing to each of the OTUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "per_OTU_samples={}\n",
    "\n",
    "samples = OTU_table.ids(axis='sample')\n",
    "\n",
    "for OTU in OTUs_as_list:\n",
    "#    print OTU\n",
    "    per_OTU_samples[OTU] = []\n",
    "    obs = OTU_table.data(OTU, axis='observation')\n",
    "    for i in range(len(obs)):\n",
    "        if int(obs[i]) > 0:\n",
    "#            print \"\\t%s\" %samples[i]\n",
    "            per_OTU_samples[OTU].append(\".\".join(samples[i].split(\".\")[:-1]))\n",
    "\n",
    "    \n",
    "print \"OTU centroid ID - # of samples\"\n",
    "for OTU in per_OTU_samples:\n",
    "    print \"%s - %s\" %(OTU,len(per_OTU_samples[OTU]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify centroid IDS for the relevant OTUs, including query target alignments ('H')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "per_OTU_centroids={}\n",
    "\n",
    "print \"OTU ids to process:\"\n",
    "for OTU in per_OTU_samples:\n",
    "    print OTU,len(per_OTU_samples[OTU])\n",
    "print \"#######\\n\"\n",
    "\n",
    "uc=open('../../2-metaBEAT/GLOBAL/global.uc', 'r')\n",
    "\n",
    "for line in uc:\n",
    "    if line.startswith('H'):\n",
    "        if line.strip().split(\"\\t\")[9] in OTUs_as_list:\n",
    "#            print \"hit: %s\\t%s\" %(line.strip().split(\"\\t\")[9],line.strip().split(\"\\t\")[8])\n",
    "            if not per_OTU_centroids.has_key(line.strip().split(\"\\t\")[9]):\n",
    "                per_OTU_centroids[line.strip().split(\"\\t\")[9]]=[line.strip().split(\"\\t\")[9]]\n",
    "\n",
    "                \n",
    "            if line.strip().split(\"\\t\")[8].split(\"|\")[0] in per_OTU_samples[line.strip().split(\"\\t\")[9]]:\n",
    "                per_OTU_centroids[line.strip().split(\"\\t\")[9]].append(line.strip().split(\"\\t\")[8])\n",
    "#                print \"found sample: %s\" %line.strip().split(\"\\t\")[8].split(\"|\")[0]\n",
    "                        \n",
    "uc.close()\n",
    "\n",
    "\n",
    "for OTU in per_OTU_centroids:\n",
    "    print OTU,str(len(per_OTU_centroids[OTU]))\n",
    "    for c in sorted(per_OTU_centroids[OTU]):\n",
    "        print \"\\tfound centroid: \"+c\n",
    "#print per_OTU_centroids\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify a unique id for each OTU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "OTUs_synonyms = []\n",
    "for sp in OTUs:\n",
    "    count=0\n",
    "#    OTUs_as_list.extend(OTUs[sp])\n",
    "    \n",
    "    for otu in OTUs[sp]:\n",
    "        OTUs_synonyms.append(sp+'_'+str(count)+'_OTU')\n",
    "        count+=1\n",
    "    \n",
    "print OTUs_as_list\n",
    "print OTUs_synonyms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify samples contributing to each of the OTUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "per_OTU_samples={}\n",
    "            \n",
    "for otu in per_OTU_centroids:\n",
    "    per_OTU_samples[otu] = []\n",
    "    for centroid in per_OTU_centroids[otu]:\n",
    "        per_OTU_samples[otu].append(centroid.split(\"|\")[0])\n",
    "#        print centroid.split(\"|\")[1]\n",
    "\n",
    "    per_OTU_samples[otu]=list(set(per_OTU_samples[otu]))\n",
    "    print \"\\n\"+otu,len(per_OTU_samples[otu]),sorted(per_OTU_samples[otu])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a global fasta file containing all reads for each OTU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "\n",
    "for i in range(len(OTUs_as_list)):\n",
    "    otu=OTUs_as_list[i]\n",
    "    syn=OTUs_synonyms[i]\n",
    "    print \"OTU: %s -> %s.fa\" %(otu,syn)\n",
    "    read_ids_per_OTU=[]\n",
    "    seqs_per_OTU=[]\n",
    "    for sample in per_OTU_samples[otu]:\n",
    "        seqs_per_sample=[]\n",
    "#        print \"Sample: %s\" %sample\n",
    "        read_ids=[]\n",
    "#        collect relevant centroids\n",
    "        centroids=[]\n",
    "        for c in per_OTU_centroids[otu]:\n",
    "#            print \"centroid: %s\" %c\n",
    "            if c.startswith(sample+'|'):\n",
    "                for c_ind in c.split(\"|\")[1:]:\n",
    "                    centroids.append(c_ind)\n",
    "                \n",
    "#        print \"Centroids: %s\" %centroids\n",
    "        \n",
    "#        extract read ids from uc file\n",
    "        read_ids.extend(centroids[:])\n",
    "        uc=open('../../2-metaBEAT/'+sample+'/'+sample+'.uc', 'r')\n",
    "        for line in uc:\n",
    "            if line.startswith('H'):\n",
    "                if line.strip().split(\"\\t\")[9] in centroids:\n",
    "#                    print line\n",
    "                    read_ids.append(line.strip().split(\"\\t\")[8])\n",
    "        uc.close()            \n",
    "#        print \"READ IDS: %i\" %len(read_ids)\n",
    "        \n",
    "#        extract reads per sample\n",
    "        \n",
    "        fasta=open('../../2-metaBEAT/'+sample+'/'+sample+'_queries.fasta', 'r')     # trimmed.fasta', 'r')\n",
    "        for r in SeqIO.parse(fasta, 'fasta'):\n",
    "            if r.id in read_ids:\n",
    "                r.id = sample+'|'+r.id\n",
    "                r.description = r.id\n",
    "                seqs_per_OTU.append(r)\n",
    "                \n",
    "            \n",
    "        fasta.close()\n",
    "        \n",
    "        \n",
    "#    Write out global fasta per OTU, containing all reads across all samples\n",
    "    out=open(syn+'.fa', 'w')\n",
    "    SeqIO.write(seqs_per_OTU, out, 'fasta')\n",
    "    out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dereplicate all sequences at 95% identity match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "for OTU in $(ls -1 *.fa)\n",
    "do\n",
    "    vsearch --derep_fulllength $OTU \\\n",
    "    --strand both --output derep_$OTU \\\n",
    "    --uc derep_$OTU.uc --id 0.95\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each OTU, extract the most abundant dereplicated sequence as reference. Compare all other sequences to this one via `usearch_global`. Parse output and reverse complement any sequences if necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract top sequence and write to file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "\n",
    "for i in range(len(OTUs_as_list)):\n",
    "    otu=OTUs_as_list[i]\n",
    "    syn=OTUs_synonyms[i]\n",
    "    print syn\n",
    "    seqs = SeqIO.parse(open('derep_'+syn+'.fa','r'),'fasta')\n",
    "\n",
    "    seq = seqs.next()\n",
    "\n",
    "#    seq.seq = seq.seq.reverse_complement()\n",
    "\n",
    "    out=open('derep_'+syn+'.ref.fasta','w')\n",
    "    SeqIO.write(seq, out, 'fasta')\n",
    "    out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare all sequences against the most abundant dereplicated sequences with `usearch_global`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "for ref in $(ls -1 derep* | grep \"fasta\")\n",
    "do\n",
    "    full=$(echo -e \"$ref\" | sed 's/\\.ref//' | sed 's/sta$//')\n",
    "    \n",
    "    vsearch --usearch_global $full \\\n",
    "    --strand both \\\n",
    "    --db $ref \\\n",
    "    --id 0.9 \\\n",
    "    --blast6out $full.blast.out \n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse output and identify sequences to reverse complement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "to_reverse = {}\n",
    "\n",
    "for i in range(len(OTUs_as_list)):\n",
    "    otu=OTUs_as_list[i]\n",
    "    syn=OTUs_synonyms[i]\n",
    "    print syn\n",
    "    to_reverse[syn] = []\n",
    "    blast = open('derep_'+syn+'.fa.blast.out','r')\n",
    "\n",
    "    for rec in blast:\n",
    "        cols = rec.strip().split(\"\\t\")\n",
    "        if cols[6] > cols[7]:\n",
    "            to_reverse[syn].append(cols[0])\n",
    "        \n",
    "    print to_reverse[syn]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reverse complement if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "\n",
    "for i in range(len(OTUs_as_list)):\n",
    "    otu=OTUs_as_list[i]\n",
    "    syn=OTUs_synonyms[i]\n",
    "    print syn\n",
    "    if to_reverse[syn]:\n",
    "        seqs = SeqIO.parse(open('derep_'+syn+'.fa','r'),'fasta')\n",
    "        outseqs = []\n",
    "        for s in seqs:\n",
    "            print \"search: %s\" %s.id\n",
    "            if s.id in to_reverse[syn]:\n",
    "                print \"#%s\\t%s\" %(to_reverse[syn].index(s.id), to_reverse[syn][to_reverse[syn].index(s.id)])\n",
    "                s.seq = s.seq.reverse_complement()\n",
    "                del(to_reverse[syn][to_reverse[syn].index(s.id)])\n",
    "                outseqs.append(s)\n",
    "        out = open('derep_'+syn+'.fa','w')\n",
    "        SeqIO.write(outseqs, out, 'fasta')\n",
    "        out.close()\n",
    "    else:\n",
    "        print \"nothing to reverse\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write top ten sequences to file and align for manual inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "\n",
    "for i in range(len(OTUs_as_list)):\n",
    "    otu=OTUs_as_list[i]\n",
    "    syn=OTUs_synonyms[i]\n",
    "    print syn\n",
    "    seqs = SeqIO.parse(open('derep_'+syn+'.fa','r'),'fasta')\n",
    "    count=0\n",
    "    seqs_to_print = []\n",
    "    for s in seqs:\n",
    "        print count,s.id\n",
    "        count+=1\n",
    "        seqs_to_print.append(s)\n",
    "        if count > 9:\n",
    "            break\n",
    "    out=open('derep_'+syn+'.top10.fa','w')\n",
    "    SeqIO.write(seqs_to_print,out,'fasta')\n",
    "    out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Align top 10 with mafft."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "for file in $(ls -1 *top10*)\n",
    "do\n",
    "    out=$(echo -e \"$file\" | sed 's/fa$/aln.fa/')\n",
    "    mafft --localpair --maxiterate 1000 $file > $out\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representatives of observed OTUs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Curated minibarcode (Leray CO1 region) sequences were selected as representatives and saved to two separate files (one per taxa)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%file Crangonyx_floridanus_OTU.minibc.ref.fasta\n",
    ">CH101-pl1-1-1-Oct-nc|1_2105_17136_11806_1_ex\n",
    "tttagcatctacagctgctcatagaggtgcttctgtagacttagctattttctctcttcacctagcaggtgcctcctctattttaggttcaattaactttatttccacagtaataaatatacgagtaaaaaatatattaatagaccaaatccctttatttgtttgagctattttcttcactactattcttcttcttcttctttctttacctgttctagcaggagctatcacaatacttttaacagaccgtaatctcaatacatcattctttgacccttctggggggggtgaccctatcttgtaccagcatctctt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%file Crangonyx_pseudogracilis_OTU.minibc.ref.fasta\n",
    ">CH304-pl1-3-7-Oct-nc|1_1106_17586_25029_1_ex\n",
    "ctctatcatcaataacagcccacagaggttcatcagtagacctggctattttttctctccacctagctggtgcatcctcaattttaggagctatcaattttctatccacaataataaatataaaagtaaaaaaccttcttatagaccaagttcctttatttgtttgagcaattttttttacaacaattcttctccttctgtctctacctgttttagccggagctatcactatactattgacagaccgcaatcttaatacatcattctttgatccatcaggaggtggagaccctattctatatcaacatctttt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing all sequences against the minibc reference sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create blast databases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "makeblastdb -in Crangonyx_floridanus_OTU.minibc.ref.fasta -dbtype nucl -out Crangonyx_floridanus_minibc_ref\n",
    "makeblastdb -in Crangonyx_pseudogracilis_OTU.minibc.ref.fasta -dbtype nucl -out Crangonyx_pseudogracilis_minibc_ref"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Blast on all *_OTU.fa* files, excluding all the dereplicated files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "for file in $(ls -1 *.fa | grep \"derep\" -v)\n",
    "do\n",
    "    prefix=$(echo -e \"$file\" | sed 's/\\.fa$//')\n",
    "    sp=$(echo -e \"$file\" | cut -d \"_\" -f 1,2)\n",
    "        \n",
    "    blastn -db $sp\\_minibc_ref -query $file -outfmt 6 -out $prefix.vs.minibc.blastn.out\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse outputs and identify clipping points for sequences longer than the reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "full_length=313\n",
    "\n",
    "\n",
    "global_clips = {}\n",
    "\n",
    "for i in range(len(OTUs_as_list)):\n",
    "    otu=OTUs_as_list[i]\n",
    "    syn=OTUs_synonyms[i]\n",
    "    print syn\n",
    "    global_clips[syn] = {}\n",
    "    blast = open(syn+'.vs.minibc.blastn.out','r')\n",
    "    \n",
    "    for OTU in blast:\n",
    "        cols=OTU.strip().split(\"\\t\")\n",
    "        temp = []\n",
    "    #    print \"#%s\" %cols[0]\n",
    "        if int(cols[8]) < int(cols[9]):\n",
    "    #        print \"\\torientation ok\"\n",
    "            temp.append(0)\n",
    "            #check the first end for overhang\n",
    "            if int(cols[6]) == 1: #sequence alignment starts at position 1 -> lower clipping point is 0, i.e. not needed\n",
    "    #            print \"\\t1 - first end starts with 1 - no clipping on this side\"\n",
    "                temp.append(0)\n",
    "            else: #Alignment does not start at position 1, then specify clippoint to clip to same length as ref\n",
    "                temp.append(int(cols[6])-int(cols[8]))\n",
    "    #            print \"\\t1 - Alignment starts at pos %s vs. %s in ref - clip at: %s\" %(cols[6],cols[8],temp[-1])\n",
    "            \n",
    "            #check second end for overhang\n",
    "            if int(cols[9]) == full_length:\n",
    "                temp.append(int(cols[7]))\n",
    "    #            print \"\\t2 - Full lenght alignment ending at pos %s in query\" %cols[7]\n",
    "            else:\n",
    "                temp.append((full_length-int(cols[9]))+int(cols[7])) \n",
    "    #            print \"\\t2 - incomplete alignment ends with query pos %s and ref pos %s - clip at: %s\" %(cols[7], cols[9], temp[-1])\n",
    "        \n",
    "        \n",
    "        else:\n",
    "    #        print \"\\treverse complement\"\n",
    "            temp.append(1)\n",
    "            if int(cols[6]) == 1: #sequence alignment starts at position 1 -> lower clipping point is 0, i.e. not needed\n",
    "    #            print \"\\t1 - first end alignment starts at base 1 - no clipping on this side\"\n",
    "                temp.append(0)\n",
    "            else:\n",
    "                temp.append(int(cols[6])-(full_length-int(cols[8]))-1)\n",
    "    #            print \"\\t1 - alignment starts with query pos %s and ref pos %s - clip at: %s\" %(cols[6], cols[8], temp[-1])\n",
    "            \n",
    "            if int(cols[9]) == 1:\n",
    "    #            print \"\\t2 - full length alignment ending in pos %s in query\" %cols[7]\n",
    "                temp.append(int(cols[7]))\n",
    "            else:\n",
    "                temp.append(int(cols[7])+int(cols[9]))\n",
    "    #            print \"\\t2 - incomplete alignment ends with query %s at ref pos %s - clip at: %s\" %(cols[7], cols[9], temp[-1])\n",
    "\n",
    "        for i in range(len(temp)):\n",
    "            if temp[i] < 0:\n",
    "                temp[i] = 0\n",
    "            \n",
    "#    print \"\\t\"+str(temp)\n",
    "        global_clips[syn][cols[0]] = temp[:]\n",
    "    \n",
    "#print global_clips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clip and reverese complement based on blast results if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "\n",
    "for i in range(len(OTUs_as_list)):\n",
    "    otu=OTUs_as_list[i]\n",
    "    syn=OTUs_synonyms[i]\n",
    "    print syn\n",
    "\n",
    "    outseqs = []\n",
    "    seqs = SeqIO.parse(open(syn+'.fa','r'),'fasta')\n",
    "\n",
    "    for s in seqs:\n",
    "        s.seq = s.seq[global_clips[syn][s.id][1]:global_clips[syn][s.id][2]]\n",
    "        if global_clips[syn][s.id][0] == 1:\n",
    "            s.seq = s.seq.reverse_complement()\n",
    "        \n",
    "        outseqs.append(s)\n",
    "    \n",
    "    out = open(syn+'.clipped.fasta','w')\n",
    "    SeqIO.write(outseqs,out,'fasta')\n",
    "    out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each OTU bin reads per sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "\n",
    "for i in range(len(OTUs_as_list)):\n",
    "    otu=OTUs_as_list[i]\n",
    "    syn=OTUs_synonyms[i]\n",
    "    print syn\n",
    "    outseqs = {}\n",
    "    \n",
    "    seqs = SeqIO.parse(open(syn+'.clipped.fasta','r'),'fasta')\n",
    "    for s in seqs:\n",
    "        sample = s.id.split(\"|\")[0]\n",
    "        \n",
    "        if not sample in outseqs:\n",
    "            outseqs[sample] = []\n",
    "        outseqs[sample].append(s)\n",
    "        \n",
    "    for sample in outseqs:\n",
    "        print \"\\twriting data for sample: %s\" %sample\n",
    "        out=open(sample+'.'+syn+'.clipped.fasta','w')\n",
    "        SeqIO.write(outseqs[sample],out,'fasta')\n",
    "        out.close()\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Identify most abundant OTU per sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import shlex, subprocess\n",
    "\n",
    "for i in range(len(OTUs_synonyms)):\n",
    "    print OTUs_synonyms[i],OTUs_as_list[i]\n",
    "    syn=OTUs_synonyms[i]\n",
    "    hts_per_OTU=[]\n",
    "    seqs_per_OTU=[]\n",
    "    \n",
    "    for f in glob.glob('*.'+syn+'.clipped.fasta'):\n",
    "        prefix = f.replace('.fasta','')\n",
    "        print \"#\"+prefix,\n",
    "\n",
    "        #cluster at 100% similarity full length\n",
    "\n",
    "        mb.vsearch_cluster_full_length(infile=f, cluster_match=float(1), threads=3, sampleID=prefix)\n",
    "        \n",
    "        #output original centroid id as chosen by vsearch\n",
    "        cs=[]\n",
    "        for c in per_OTU_centroids[OTUs_as_list[i]]:\n",
    "            if f.split(\".\")[0] == c.split(\"|\")[0]:\n",
    "                cs.append(c)\n",
    "        print \" - %s\" %cs,\n",
    "                \n",
    "        hts_per_OTU.append(mb.find_most_abundant_seq_from_uc(uc=prefix+'.uc'))\n",
    "        print \" -> %s\" %hts_per_OTU[-1],\n",
    "        \n",
    "        if hts_per_OTU[-1] in cs:\n",
    "            print \" - OK\"\n",
    "        else:\n",
    "            print \" - adjust\"\n",
    "        \n",
    "\n",
    "#        os.remove(f+'.uc')\n",
    "#        os.remove(f+'_centroids.fasta')\n",
    "#        os.remove(f)\n",
    "\n",
    "    print \"extracting hts for %s -> %s\" %(syn,syn+'_hts.fasta')\n",
    "    for r in SeqIO.parse(syn+'.clipped.fasta', 'fasta'):\n",
    "        if r.id in hts_per_OTU:\n",
    "            seqs_per_OTU.append(r)\n",
    "                \n",
    "    fasta.close()\n",
    "#    print \"final cleanup .. \",\n",
    "#    os.remove(OTUs_synonyms[i]+'.fa')\n",
    "#    print \"DONE!\\n\"\n",
    "        \n",
    "    out=open(syn+'_hts.fasta', 'w')\n",
    "    SeqIO.write(seqs_per_OTU, out, 'fasta')\n",
    "    out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each original OTU, cluster the chosen (most abundant) haplotypes for each sample at 100% to remove redundancy and identify the set of unique observed haplotypes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for f in glob.glob('*OTU_hts.fasta'):\n",
    "    print f\n",
    "    prefix = f.replace('.fasta','')\n",
    "    mb.vsearch_cluster_full_length(infile=f, cluster_match=float(1), threads=3, sampleID=prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Align the non-redundant sequences with mafft."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "for file in $(ls -1 *_hts_centroids*)\n",
    "do\n",
    "    out=$(echo -e \"$file\" | sed 's/fasta$/aln.fasta/')\n",
    "    mafft --localpair --maxiterate 1000 $file > $out\n",
    "done\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manually inspect alignments and remove dubious bases. Curated alignments were saved to directory `haplotype_alignments`.\n",
    "Cluster observed haplotypes again at 100% similarity to remove redundancy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "import glob\n",
    "\n",
    "for f in glob.glob('../haplotypes_alignments/*'):\n",
    "    print f\n",
    "    prefix = f.split(\"/\")[1].replace(\"_centroids.aln.fasta\",\"\")\n",
    "    print prefix\n",
    "    out = \"\"\n",
    "    seqs = SeqIO.parse(open(f,'r'), 'fasta')\n",
    "    for s in seqs:\n",
    "        out+=\">%s\\n%s\\n\" %(s.id,str(s.seq).replace(\"-\",\"\").upper())\n",
    "\n",
    "    fh = open(prefix+'.fasta','w')\n",
    "    fh.write(out)\n",
    "    fh.close()\n",
    "    \n",
    "    mb.vsearch_cluster_full_length(infile=prefix+'.fasta', cluster_match=float(1), threads=3, sampleID=prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Give unique names to haplotypes and write to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "import glob\n",
    "\n",
    "for f in glob.glob('*_hts_centroids.fasta'):\n",
    "    print f\n",
    "    sp_prefix = f.split(\"_\")[0][0]+f.split(\"_\")[1][0]\n",
    "    prefix = f.replace(\"_centroids.fasta\",\"\")\n",
    "    print prefix\n",
    "    \n",
    "    count = 1\n",
    "\n",
    "    seqs = SeqIO.parse(open(f, 'r'), 'fasta')\n",
    "\n",
    "    fh=open(prefix+'.nr.fasta','w')\n",
    "    for s in seqs:\n",
    "        fh.write(\">\"+sp_prefix+'_UK_Mb-'+\"%02d\\n%s\\n\" %(count,s.seq))\n",
    "        count+=1\n",
    "    fh.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenate all haplotypes into a single file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "import glob\n",
    "\n",
    "seqs = []\n",
    "\n",
    "for i in range(len(OTUs_synonyms)):\n",
    "    print OTUs_synonyms[i],OTUs_as_list[i]\n",
    "    syn=OTUs_synonyms[i]\n",
    "    sp=\"_\".join(syn.split(\"_\")[:2])\n",
    "    print sp\n",
    "#    prefix = sp.split(\"_\")[0][0]+sp.split(\"_\")[1][0]\n",
    "#    print prefix\n",
    "    for r in SeqIO.parse(open(syn+'_hts.nr.fasta','r'), 'fasta'):\n",
    "        r.description = sp+'|'+r.id\n",
    "        r.id = r.description\n",
    "        seqs.append(r)\n",
    "        \n",
    "out=open('Crangonyx_from_metaBEAT.fasta', 'w')\n",
    "SeqIO.write(seqs, out, 'fasta')\n",
    "out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the observed haplotypes to the full set of sequences to identify the samples which contain sequences that receive full length hits from the haplotypes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "for s in $(ls -1 *_hts.nr.fasta)\n",
    "do\n",
    "    prefix=$(echo -e \"$s\" | cut -d \".\" -f 1 | sed 's/_hts//')\n",
    "    vsearch --usearch_global $s  \\\n",
    "    --strand both \\\n",
    "    --db $prefix.clipped.fasta \\\n",
    "    --id 0.97 --query_cov 1 --maxaccepts 100000 \\\n",
    "    --blast6out $prefix.vs.full.blast.out \n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove redundancy from Sanger sequences produced with Folmer primers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "vsearch --cluster_fast ../../4-infer_phylogeny/sequences/c.flor_c.pse_SANGER_full.fasta --id 1.0 --strand both --threads 3 \\\n",
    "--centroids ../../4-infer_phylogeny/sequences/c.flor_c.pse_SANGER.nr.fasta \\\n",
    "--uc ../../4-infer_phylogeny/sequences/c.flor_c.pse_SANGER.nr.uc --query_cov 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
